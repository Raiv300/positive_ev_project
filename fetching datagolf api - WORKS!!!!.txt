"""
FINAL WORKING DataGolf Fetcher
Step 1: Get all events, Step 2: Get odds for each event
"""
import requests
import pandas as pd
import os
import time
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()
API_KEY = os.getenv("DATAGOLF_API_KEY")

print("="*70)
print("FETCHING PGA DATA FROM DATAGOLF (2-STEP PROCESS)")
print("="*70)

BASE_URL = "https://feeds.datagolf.com"

# ============================================================================
# STEP 1: Get list of all events for 2023-2024
# ============================================================================
print("\n[STEP 1] Getting event list for 2023-2024...")

event_url = f"{BASE_URL}/historical-raw-data/event-list"
all_events = []

for year in [2023, 2024]:
    print(f"\n  Fetching {year} events...")
    
    params = {
        "tour": "pga",
        "year": year,
        "file_format": "json",
        "key": API_KEY
    }
    
    try:
        r = requests.get(event_url, params=params, timeout=30)
        
        if r.status_code == 200:
            data = r.json()
            
            # Might be list or dict with 'events' key
            if isinstance(data, list):
                events = data
            elif isinstance(data, dict) and 'events' in data:
                events = data['events']
            else:
                print(f"    ⚠️  Unexpected format: {list(data.keys())}")
                continue
            
            all_events.extend(events)
            print(f"    ✓ Got {len(events)} events for {year}")
            
        else:
            print(f"    ❌ Error {r.status_code}")
            
    except Exception as e:
        print(f"    ❌ Error: {e}")

if len(all_events) == 0:
    print("\n❌ NO EVENTS FOUND!")
    print("\nYour API might not have access to event lists.")
    print("Trying alternative: fetch most recent events...")
    
    # Fallback: try to get current events
    schedule_url = f"{BASE_URL}/get-schedule"
    params = {"tour": "pga", "file_format": "json", "key": API_KEY}
    
    r = requests.get(schedule_url, params=params, timeout=30)
    if r.status_code == 200:
        data = r.json()
        if isinstance(data, dict) and 'schedule' in data:
            all_events = data['schedule']
            print(f"  ✓ Got {len(all_events)} events from schedule")
        elif isinstance(data, list):
            all_events = data
            print(f"  ✓ Got {len(all_events)} events from schedule")
    
    if len(all_events) == 0:
        print("\n❌ CANNOT GET EVENT LIST")
        print("\nYour DataGolf API subscription might not include historical data access.")
        print("\nOptions:")
        print("  1. Upgrade your DataGolf subscription")
        print("  2. Use The Odds API instead (I'll give you code)")
        print("  3. Download historical data from Kaggle/other sources")
        exit(1)

print(f"\n✓ Total events found: {len(all_events)}")

# Show sample
if len(all_events) > 0 and isinstance(all_events[0], dict):
    print(f"  Sample event keys: {list(all_events[0].keys())}")

# ============================================================================
# STEP 2: Fetch odds for each event
# ============================================================================
print(f"\n[STEP 2] Fetching odds for {len(all_events)} events...")
print("(This will take a while - about 1 second per event)\n")

odds_url = f"{BASE_URL}/historical-odds/outrights"
all_odds_rows = []

for i, event in enumerate(all_events[:10], 1):  # Start with first 10 events as test
    # Extract event_id (might be 'event_id', 'id', or other field)
    event_id = event.get('event_id') or event.get('id') or event.get('tournament_id')
    event_name = event.get('event_name') or event.get('name') or f"Event {event_id}"
    event_year = event.get('year') or event.get('season')
    
    if not event_id:
        print(f"  [{i}/{len(all_events)}] Skipping - no event_id")
        continue
    
    print(f"  [{i}/{len(all_events)}] {event_name} (ID: {event_id})...")
    
    for book in ["draftkings"]:  # Start with just DraftKings
        for market in ["win"]:  # Start with just win market
            
            params = {
                "tour": "pga",
                "event_id": event_id,
                "market": market,
                "book": book,
                "odds_format": "american",
                "file_format": "json",
                "key": API_KEY
            }
            
            try:
                r = requests.get(odds_url, params=params, timeout=30)
                
                if r.status_code != 200:
                    print(f"    ⚠️  {book}/{market}: Error {r.status_code}")
                    continue
                
                data = r.json()
                
                # Extract odds list from response
                if isinstance(data, dict) and 'odds' in data:
                    odds_list = data['odds']
                    
                    # Add metadata to each row
                    for odds_row in odds_list:
                        odds_row['event_id'] = event_id
                        odds_row['event_name'] = event_name
                        odds_row['year'] = event_year
                        odds_row['tour'] = 'pga'
                        odds_row['book'] = book
                        odds_row['market'] = market
                        all_odds_rows.append(odds_row)
                    
                    print(f"    ✓ {book}/{market}: {len(odds_list)} players")
                else:
                    print(f"    ⚠️  {book}/{market}: Unexpected format")
                
                time.sleep(0.5)  # Be nice to API
                
            except Exception as e:
                print(f"    ❌ Error: {e}")
                continue

# ============================================================================
# STEP 3: Save results
# ============================================================================
print("\n" + "="*70)
print("FETCH COMPLETE")
print("="*70)

if len(all_odds_rows) == 0:
    print("\n❌ NO ODDS DATA FETCHED!")
    exit(1)

print(f"\nTotal rows fetched: {len(all_odds_rows):,}")

# Convert to DataFrame
df = pd.DataFrame(all_odds_rows)

print(f"\nDataFrame shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")

# Save
output_path = Path("data/interim/player_odds_data.csv")
output_path.parent.mkdir(parents=True, exist_ok=True)

df.to_csv(output_path, index=False)

file_size_mb = output_path.stat().st_size / 1024 / 1024

print(f"\n✅ SUCCESS!")
print(f"  Saved to: {output_path}")
print(f"  File size: {file_size_mb:.2f} MB")
print(f"  Total rows: {len(df):,}")

# Show summary
print(f"\nData summary:")
if 'year' in df.columns:
    print(f"  Years: {df['year'].value_counts().sort_index().to_dict()}")
if 'event_name' in df.columns:
    print(f"  Events: {df['event_name'].nunique()}")
if 'player_name' in df.columns:
    print(f"  Unique players: {df['player_name'].nunique()}")

print(f"\nSample data:")
print(df.head(3))

if len(df) >= 500:
    print(f"\n✓ Good data volume! Now run 01_DataCleaning.ipynb")
else:
    print(f"\n⚠️  Only {len(df)} rows - you might want to fetch more events")
    print(f"    Remove the [:10] limit on line 124 to fetch all events")

print("="*70)